# Airflow Study QA Log

이 파일은 우리 학습 세션의 모든 질문과 답변을 기록하는 공간입니다. 맥락을 유지하고 효율적으로 소통하기 위해 사용합니다.

---

## [2026-01-26] Airflow 환경 및 워크플로우 가이드

**질문**: Airflow를 실행하는 법과 WSL, VS Code, 가상환경을 어떻게 활용해야 하는지 알려줘.

**답변**: 시니어 데이터 엔지니어로서 실무에서 사용하는 가장 효율적인 워크플로우를 정리해 드립니다.

### 1. WSL vs VS Code 역할 분담
- **WSL (Ubuntu Terminal)**: **"서버의 엔진"** 역할을 합니다. Airflow의 스케줄러와 웹서버를 실제로 실행하는 공간입니다. 명령어를 입력하고 로그를 확인하는 용도로 씁니다.
- **VS Code**: **"조종석"** 역할을 합니다. WSL에 원격 접속(Remote - WSL 확장 프로그램)하여 `dags/` 폴더 내의 파이썬 코드를 편집합니다. 우리가 수정한 코드는 저장 즉시 WSL 내부 파일 시스템에 반영됩니다.

### 2. 가상환경 및 Airflow 실행 순서
WSL 터미널에서 아래 순서대로 진행해 보세요.

1. **가상환경 활성화**: 
   ```bash
   source .venv/bin/activate  # 또는 설정하신 가상환경 경로
   ```
2. **Airflow 실행 (Standalone 모드)**: 
   초급/중급 단계에서는 관리가 편한 standalone 명령어를 추천합니다.
   ```bash
   airflow standalone
   ```
   *이후 터미널에 출력되는 `admin` 비밀번호와 웹 주소(보통 `localhost:8080`)를 확인하세요.*

### 3. 개발 워크플로우
1. **VS Code**에서 새로운 `.py` 파일을 `dags/` 폴더에 생성/수정합니다.
2. **WSL**에서 실행 중인 Airflow가 해당 파일을 자동으로 감지(Parsing)합니다.
3. 브라우저(`localhost:8080`)에서 **Airflow UI**를 새로고침하여 DAG가 뜨는지 확인하고 실행(Trigger)합니다.

> [!TIP]
> 실무에서는 터미널을 두 개 띄워 하나는 `airflow scheduler`, 다른 하나는 `airflow webserver`를 실행하지만, 학습용으로는 `airflow standalone` 하나면 충분합니다.

---

## [2026-01-26] Airflow 실행 명령어 (Docker vs Standalone)

**질문**: `sudo docker compose up`이 제가 실행해야 하는 명령어인가요?

**답변**: 현재 환경에서는 **어떻게 설치했느냐**에 따라 다릅니다.

1. **Standalone 모드 (추천)**:
   - 만약 가상환경(`.venv`)에 `pip install apache-airflow`로 설치했다면, WSL 터미널에서 `airflow standalone` 명령어를 사용합니다.
   - 현재 디렉터리 구조상 Docker 설정 파일이 보이지 않으므로, 이 방식일 확률이 높습니다.

2. **Docker Compose 모드**:
   - `docker-compose.yaml` 파일이 있는 위치에서 `sudo docker compose up`을 실행합니다.
   - 이 방식은 데이터베이스(Postgres 등)와 Redis를 별도의 컨테이너로 띄워 실제 운영 환경과 매우 흡사하게 구성할 때 씁니다.

**확인 방법**:
- WSL 터미널에서 `airflow version`을 입력해 보세요. 버전이 잘 나온다면 가상환경 내에서 `airflow standalone`으로 시작하시면 됩니다.
- 만약 에러가 난다면 Docker 방식을 사용 중일 수 있으니, `docker-compose.yml` 파일이 어디 있는지 확인이 필요합니다.

> [!NOTE]
> `sudo`는 Docker 권한 문제 때문일 수 있지만, 가상환경 내에서 직접 `airflow`를 실행할 때는 보통 `sudo`를 쓰지 않습니다.

---

## [2026-01-26] 안티그래비티(AI)의 역할 및 파일 편집 방식

**질문**: 서버는 실행했습니다. VS Code로 파일을 직접 편집해야 하나요? 안티그래비티는 파일을 못 만드나요?

**답변**: **제가 전부 다 할 수 있습니다!** 제가 이 프로젝트의 "시니어 데이터 엔지니어" 동료라고 생각하시면 됩니다.

1. **파일 생성 및 편집**: 제가 직접 `dags/`, `plugins/` 폴더 등에 파일을 만들고 수정할 수 있습니다. 사용자님은 VS Code를 켜서 제가 만든 코드를 **확인**만 하셔도 충분합니다.
2. **자동 반영**: 제가 파일을 만들거나 수정하면, 현재 실행 중인 Docker 컨테이너의 Airflow가 **자동으로** 그 변화를 감지합니다. (보통 30초 내외)
3. **학습 방식 제안**: 
   - 제가 가상의 시나리오(과제)를 드리면, 직접 VS Code에서 작성해 보시고 저에게 **리뷰 요청**을 하시는 것이 학습 효과가 가장 좋습니다.
   - 하지만 원하신다면 제가 **뼈대 코드(Boilerplate)**를 먼저 짜드리고 설명해 드릴 수도 있습니다.

## [2026-01-26] Session 01: PythonOperator (5분 핵심 요약)

**개념 설명 요청**: PythonOperator의 역할과 활용법

**답변**: Airflow에서 데이터 분석가가 가장 많이, 그리고 가장 먼저 마주하게 될 연산자입니다.

### 1. 역할 (언제 쓰는가?)
- **"파이썬 함수를 Task로 실행하고 싶을 때"** 사용합니다.
- 데이터 정제, 산식 계산, API 호출 후 결과 파싱 등 파이썬으로 할 수 있는 모든 로직을 DAG의 한 단계로 만들 수 있습니다.

### 2. 핵심 파라미터 2가지
```python
from airflow.operators.python import PythonOperator

def my_function(name, **context):
    print(f"Hello {name}")

task_1 = PythonOperator(
    task_id='print_name_task',
    python_callable=my_function,       # 실행할 파이썬 함수 이름
    op_args=['Antigravity'],           # 함수에 넘길 일반 인자 (리스트)
    # op_kwargs={'name': 'Antigravity'} # 함수에 넘길 키워드 인자 (딕셔너리)
)
```

### 3. 실무 시니어의 조언
- **언제 쓰지 말아야 하는가?**: 
  - 수 기가바이트(GB) 규모의 데이터를 메모리에 다 올리는 로직은 위험합니다. (Airflow Worker의 메모리를 다 잡아먹어 서버가 터질 수 있음)
  - 대용량 데이터는 SQL(Query)로 처리하거나 Spark 같은 외부 연산 엔진을 쓰는 게 정석입니다.
- **Task 책임 분리**:
  - `데이터 수집 + 정제 + 저장`을 하나의 함수에 다 넣지 마세요.
  - 수집 Task, 정제 Task, 저장 Task로 **나눠야** 어디서 실패했는지 바로 알 수 있고, 실패한 부분부터 재시작(Retry)할 수 있습니다.

---

## [2026-01-26] 코드 동기화 방식 (로컬 개발 vs 수업/운영 환경)

**질문**: 서버는 따로 띄우고, 코드는 Git Push/Pull로만 업로그했었는데 여기서는 바로 반영이 안 되나요? 어떤 게 맞는 방식인가요?

**답변**: 둘 다 맞지만 **상황에 따라 다릅니다.** 시니어 엔지니어가 명확히 정리해 드릴게요.

### 1. 수업/운영 환경 (Git 중심 워크플로우)
- **방식**: `내 PC(로컬) -> Git Push -> 원격 서버 -> Git Pull -> Airflow 반영`
- **이유**: 한 대의 서버를 여러 명이 공유하거나, 운영 환경의 안정성을 위해 "검증된 코드(Git)"만 올리기 위해서입니다. 수업 때 하신 방식이 바로 이 "협업/운영 표준"입니다.

### 2. 현재 우리 학습 환경 (원격 서버 + Git 워크플로우)
- **방식**: `내 PC에서 코드 작성 -> Git Push -> 원격 서버에서 Git Pull -> Airflow 반영`
- **중요**: 사용자님 말씀이 맞습니다! 현재 원격 서버를 사용 중이시라면, 제가 여기서 파일을 생성해도 **원격 서버에 직접 전달되지는 않습니다.** 
- **작업 흐름**: 
  1. 제가 여기서 코드를 짜드리면, 사용자님이 그 코드를 복사해서 **내 PC의 VS Code**에 붙여넣습니다.
  2. 그 파일을 **Git Push** 하여 원격 서버로 보냅니다.
  3. 원격 서버에서 **Git Pull**을 통해 파일을 업데이트하면 Airflow가 이를 감지합니다.

> [!IMPORTANT]
> 제가 사용자님의 원격 서버 계정이나 Git 권한을 직접 가지고 있지 않기 때문에, 제가 만든 코드를 **"배달"하는 역할은 사용자님**이 해주셔야 합니다. 저는 "설계도"를 완벽하게 그려드리는 역할에 집중하겠습니다.

### 3. 왜 바로 안 뜰까? (Sync & Parsing Delay)
- `Git Pull` 이후에도 Airflow 스케줄러가 파일을 읽는 데 **약 30초~1분** 정도 시간이 걸릴 수 있습니다.
- UI 상단에 파란색 배너로 "New DAGs detected" 같은 메시지가 뜨면 반영된 것입니다.

---

## [2026-01-26] WSL 환경 분석 및 Git 연결 가이드

**질문**: 현재 WSL에서 `ls -al`을 찍어보니 `dags`, `docker-compose.yaml` 등이 나옵니다. 지금 제 상태가 뭐고, 깃 클론을 해야 하나요?

**답변**: 현재 상태를 정확히 진단해 드립니다.

### 1. 현재 상태 진단
- **장소**: WSL의 홈 디렉터리(`~`)에 계십니다.
- **도구**: `docker-compose.yaml`이 있는 것으로 보아, 이미 Airflow를 Docker로 설치해 두신 상태입니다.
- **문제**: 하지만 현재 이 폴더는 **Git과 연결되어 있지 않습니다.** (리스트에 `.git` 폴더가 보이지 않습니다.) 
- **결론**: 제가 작업 중인 **내 PC(Windows)의 코드**를 이 **WSL(서버)**로 보내려면, 두 공간을 이어줄 **'통로(Git Remote)'**가 필요합니다.

### 2. 해결 방법: 깃 연결하기
가장 추천하는 방식은 **GitHub**(또는 수업 때 쓰신 Git 서버)을 경유지로 쓰는 것입니다.

1. **GitHub에 저장소(Repository) 만들기**: GitHub에서 `airflow-study` 같은 빈 저장소를 하나 만듭니다.
2. **내 PC(Windows)에서 연결**:
   ```bash
   cd /c/vscode/airflow/airflow
   git remote add origin <GitHub_주소>
   git push origin main
   ```
3. **WSL(서버)에서 연결**:
   ```bash
   cd ~/  # WSL 홈으로 이동
   # 이미 dags 폴더가 있으므로, 빈 곳에서 새로 클론한 뒤 파일을 옮기거나 하셔야 합니다.
   git clone <GitHub_주소> temp_repo
   cp temp_repo/dags/* ~/dags/
   ```

### 3. 더 쉬운 방법 (로컬 전용)
만약 원격 서버가 아니라 **내 컴퓨터 안의 WSL**이라면, Git Push/Pull 없이도 파일을 옮길 수 있습니다.
- WSL 터미널에서 아래 명령어를 입력해 보세요:
  ```bash
  cp /mnt/c/vscode/airflow/airflow/dags/assignment_01_basic_dag.py ~/dags/
  ```
- 이렇게 하면 윈도우에 있는 파일을 WSL의 Airflow 폴더로 바로 복사합니다.

> [!TIP]
> 수업 때처럼 "배포"의 손맛을 느끼고 싶으시다면 **GitHub을 이용한 1번 방식**을 추천드리고, 일단 빨리 실행해보고 싶으시다면 **2번(복사) 방식**을 추천드립니다!

---

## [2026-01-26] 현재 Git 저장소 동기화 및 WSL 활용법 (종합)

**질문**: 이미 이 폴더 자체가 깃 클론을 한 곳입니다. 현재 상태에서 어떻게 이동하고 동기화하면 되나요?

**답변**: 혼선을 드려 정말 죄송합니다! 현재 폴더(`c:\vscode\airflow\airflow`)가 이미 **GeonYul2/airflow** 저장소라는 점을 확인했습니다. 

새로운 걸 만들 필요 없이, **지금 있는 이 저장소를 그대로** 쓰시면 됩니다.

### 1단계: 내 PC(Windows)에서 수정한 코드 보내기
제가 여기서 파일을 수정하거나 과제 코드를 짜드리면, 사용자님은 VS Code 터미널에서 아래만 하시면 됩니다.
```bash
git add .
git commit -m "과제 01 주석 추가 및 리팩토링"
git push origin main
```

### 2단계: WSL(서버)에서 코드 내려받기
사용자님의 WSL `ls -al` 결과에서 `airflow`라는 폴더가 보였습니다. 그곳이 바로 클론된 폴더일 것입니다.
```bash
# 1. WSL에서 에어플로우 폴더로 이동
cd ~/airflow

# 2. 윈도우에서 보낸 코드를 서버로 가져오기
git pull origin main
```

### 3단계: 파일 위치 확인 및 Airflow 반영
만약 WSL의 Airflow가 `~/dags` 폴더를 바라보고 있다면, `~/airflow/dags`에 있는 파일들을 그곳으로 복사하거나 연결(Symbolic Link)해 주어야 합니다.
```bash
# 한 번만 실행: ~/dags 폴더가 ~/airflow/dags를 바라보게 만듭니다.
rm -rf ~/dags
ln -s ~/airflow/dags ~/dags
```
이렇게 한 번만 설정해 두면, 앞으로는 **[윈도우에서 Push] -> [WSL에서 Pull]**만 하면 끝납니다!

### 💡 요약
- **이미 깃허브에 연결된 상태**이므로 새로운 저장소를 만들지 마세요.
- **지금 이 대화가 일어나고 있는 폴더**에서 바로 `git push`를 하시면 됩니다.
- WSL에서는 `cd ~/airflow`로 가서 `git pull`만 하시면 됩니다.

---

## [2026-01-26] 안티그래비티와 VS Code는 같은 파일을 보고 있나요?

**질문**: 제가 여기서 기껏 작업한 걸 깃에 푸쉬하면, VS Code에서 따로 커밋하고 푸쉬 안 해도 되는 거겠죠?

**답변**: **네, 맞습니다!** 아주 중요한 포인트를 짚어주셨습니다.

1. **한 지붕 두 가족**: 저(안티그래비티)와 사용자님의 VS Code는 지금 **`c:\vscode\airflow\airflow`라는 똑같은 폴더**를 바라보고 있습니다.
2. **실시간 반영**: 제가 여기서 파일을 만들거나 수정하면, 사용자님의 VS Code 화면에서도 그 내용이 실시간으로 슥- 바뀝니다. (반대로 사용자님이 VS Code에서 수정하시면 저도 바로 알 수 있습니다.)
3. **결론**: 제가 작업한 내용까지 포함해서 여기서 `git push`를 한 번만 성공하시면, **이미 깃허브로 모든 코드가 날아간 상태**가 됩니다. VS Code에서 따로 수작업을 하실 필요가 전혀 없습니다!

**워크플로우 요약**:
- (나/AI) "과제 코드 만들었다!" -> (사용자) "오, VS Code에 떴네? 확인 완료." -> (나/사용자) **`git push`** (한 번만!) -> (WSL) **`git pull`**

---

## [2026-01-26] Git Pull 시 충돌 발생 해결 및 동기화 완료

**질문**: `git reset --hard origin/master`를 완료했습니다. 그대로 `pull` 해도 될까요?

**답변**: **이미 동기화가 끝난 상태입니다!** 

- `git reset --hard origin/master`는 "원격 서버(GitHub)의 상태와 내 서버의 상태를 100% 똑같이 강제로 맞추라"는 명령어입니다.
- 따라서 현재 사용자님의 WSL 폴더는 이미 제가 윈도우에서 보낸 최신 코드들과 **완벽하게 일치**합니다. 따로 `pull`을 더 하지 않으셔도 `Already up to date`가 뜰 것입니다.

### ✅ 지금 확인해 볼 것
1. **WSL 파일 확인**: `ls -R` 명령어로 `learning/`, `docs/`, `GUIDE.md` 파일들이 보이는지 확인해 보세요.
2. **Airflow UI 확인**: 브라우저에서 `localhost:8080` (혹은 서버 주소)에 접속하여 `assignment_01_basic_analytics` DAG가 목록에 나타나는지 확인합니다. (나타나지 않는다면 1분 정도 기다렸다가 새로고침해 보세요.)

**축하합니다!** 이제 모든 환경 설정이 끝났습니다. 이제 즐겁게 에어플로우를 배울 준비가 다 되었습니다. 🚀

---

## [2026-01-26] Git Pull 안 했는데 왜 파일이 들어왔나요? (정확한 원리)

**질문**: `reset`하면서 자동으로 `pull`을 해줬다는 뜻인가요?

**답변**: 비슷하지만 정확히는 **"이미 가져와 놓은(Fetch) 데이터를 끄집어낸 것"**입니다. 다시 한번 아주 쉽게 비유해 볼게요.

1. **`git pull` (배달 + 상자 까기)**:
   - 아까 실행하신 `pull`은 `우체국에서 우리 집 마당으로 택배 가져오기(Fetch)` + `택배 상자 까서 방에 배치하기(Merge)` 두 단계를 한꺼번에 하려던 것입니다.
   - 그런데 방 안에 예전 짐(`dags_dash_operator.py`)이 있어서 **'상자 까기' 단계에서 에러**가 났습니다. 하지만 **'택배 상자'는 이미 우리 집 마당(현지의 비밀 공간)에 도착**한 상태였습니다.

2. **`git reset --hard origin/master` (방 싹 치우고 새 짐 깔기)**:
   - 이 명령어는 "자동으로 `pull`을 다시 시도"하는 게 아닙니다.
   - 대신 **"마당에 이미 와 있는 그 택배 상자(`origin/master`) 내용물대로, 내 방을 싹 다 갈아치워!"**라고 명령한 것입니다.
   - 그래서 깃허브 서버에 다시 가지 않고도, 마당에 있던 데이터를 써서 파일들이 최신으로 바뀐 것입니다.

**요약**: `pull`이 가져오기까진 성공했기 때문에, `reset`이 그 가져온 정보를 활용해 최종 반영을 마무리지은 것입니다.

---

## [2026-01-26] 이제 에어플로우(UI)에 반영이 된 건가요?

**질문**: 서버로 코드는 다 들어왔습니다. 그럼 이제 에어플로우에 "Push"가 된 건가요?

**답변**: 네, 기술적으로는 **"서버에 배달이 완료되었고, 에어플로우가 읽기만 하면 되는 상태"**입니다. 최종 반영 과정을 정리해 드릴게요.

### 1. 데이터의 여정 (Data Flow)
- **1단계**: 내 PC -> **Git Push** -> GitHub (구름 위 저장소)
- **2단계**: GitHub -> **Git Reset(또는 Pull)** -> WSL 서버 폴더 (`~/airflow/dags`)
- **3단계 (현재)**: WSL 서버 폴더에 파일이 놓여 있음.

### 2. 에어플로우가 인지하는 과정 (Parsing)
- Airflow UI는 데이터베이스가 아니라 **`dags/` 폴더에 있는 실제 파일**을 실시간으로 감시합니다.
- 서버의 `~/dags` 폴더에 파일이 들어온 순간, **Airflow Scheduler**가 "어? 새로운 파일이 왔네?" 하고 파일을 읽어 들입니다. (이걸 **파싱(Parsing)**이라고 합니다.)

### 3. 최종 확인 방법
- 브라우저에서 Airflow UI(`localhost:8080`)를 새로고침해 보세요.
- `assignment_01_basic_analytics`라는 이름의 DAG가 목록에 보인다면 **진짜로 Push(반영)가 성공**한 것입니다! 

**요약**: 깃 명령어를 통해 서버에 파일을 "배달"했으니, 이제 에어플로우라는 "배달원"이 그 상자를 열어보고 UI에 띄워주기만 하면 됩니다. (약 30초~1분 대기)

---
